{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOXV8LnWRww5",
    "outputId": "1bf637f5-603c-4d76-fc16-5b406fc21663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fa5IrVF_qjeA",
    "outputId": "fbd84e52-d662-46a9-aff6-211c7151f3da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.sh\n",
    "\n",
    "export CUDA_HOME=/usr/local/cuda-10.1\n",
    "git clone https://github.com/NVIDIA/apex\n",
    "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndlkBXHnsbu4",
    "outputId": "22005407-22ac-47ca-fa93-e6360715d7cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ujson\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
      "\r",
      "\u001b[K     |█▉                              | 10kB 25.0MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 20kB 32.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 30kB 34.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 40kB 23.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 51kB 15.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 61kB 16.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 71kB 15.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 81kB 16.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 92kB 17.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 102kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 112kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 122kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 133kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 143kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 153kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 163kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 174kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 184kB 14.6MB/s \n",
      "\u001b[?25hInstalling collected packages: ujson\n",
      "Successfully installed ujson-4.0.2\n",
      "Collecting wandb\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/f6/91c07f54c2162854f5028aaa13f576ca17a3bc0cf6da02c2ad5baddae128/wandb-0.10.33-py2.py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 14.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Collecting GitPython>=1.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/91/b38c4fabb6e5092ab23492ded4f318ab7299b19263272b703478038c0fbc/GitPython-3.1.18-py3-none-any.whl (170kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 50.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
      "Collecting pathtools\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Collecting subprocess32>=3.5.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 15.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
      "Collecting configparser>=3.8.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 41.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (57.0.0)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: pathtools, subprocess32\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=e38ad95ff6904ef3b06541437d7413e7e6f3431f39973b01d49b2c23abd4e135\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=9f2b3da9a661bccd00f05d41d7d818b60e11ca1209766e57891f3d21aaf7783b\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
      "Successfully built pathtools subprocess32\n",
      "Installing collected packages: smmap, gitdb, GitPython, pathtools, shortuuid, subprocess32, configparser, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.1.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.33\n",
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 14.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 47.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 51.3MB/s \n",
      "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install ujson\n",
    "!pip install wandb\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssnKYYiGslbx",
    "outputId": "89f32538-f682-4437-8e29-65ebd9b3932e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsp8kYemksQf",
    "outputId": "6e375f5d-47e1-45d7-f5a5-8b7be017bbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 8054, done.\u001b[K\n",
      "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
      "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
      "remote: Total 8054 (delta 68), reused 97 (delta 44), pack-reused 7913\u001b[K\n",
      "Receiving objects: 100% (8054/8054), 14.11 MiB | 22.51 MiB/s, done.\n",
      "Resolving deltas: 100% (5467/5467), done.\n",
      "/content\n",
      "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
      "  cmdoptions.check_install_build_global(options)\n",
      "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\n",
      "/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! git clone https://github.com/NVIDIA/apex.git\n",
    "% cd /content\n",
    "!pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZrqlfm3aO1R"
   },
   "outputs": [],
   "source": [
    "! cp -r -f /content/drive/'My Drive'/active/rus_re/ /rus_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWjOB_WqaxHX",
    "outputId": "c2e51cf8-fdf7-45bc-98bc-6f7a1d388a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-30 18:28:21.962787: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5600\n",
      "Warmup steps: 336\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [32:15<00:00, 38.71s/it]\n",
      "dir_num: 0 f1: 0.20930232558139536\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5600\n",
      "Warmup steps: 336\n",
      "100% 50/50 [33:35<00:00, 40.31s/it]\n",
      "dir_num: 1 f1: 0.26373626373626374\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5650\n",
      "Warmup steps: 339\n",
      "100% 50/50 [35:41<00:00, 42.83s/it]\n",
      "dir_num: 2 f1: 0.2748091603053435\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"0 1 2\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path bert-base-multilingual-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVLv-v87j_vF",
    "outputId": "b2131ec1-f4b0-4241-cfe4-34e9c9b3cd6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-30 20:17:43.368788: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5600\n",
      "Warmup steps: 336\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [31:57<00:00, 38.35s/it]\n",
      "dir_num: 3 f1: 0.2676056338028169\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5550\n",
      "Warmup steps: 333\n",
      "100% 50/50 [32:36<00:00, 39.13s/it]\n",
      "dir_num: 4 f1: 0.25477707006369427\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5750\n",
      "Warmup steps: 345\n",
      "100% 50/50 [35:03<00:00, 42.08s/it]\n",
      "dir_num: 5 f1: 0.18604651162790695\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"3 4 5\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path bert-base-multilingual-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e64nJ2ylkA4G",
    "outputId": "9d3f2caa-5ebe-4af6-afa0-f245eeae88ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-30 22:06:33.455838: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5450\n",
      "Warmup steps: 327\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [31:18<00:00, 37.57s/it]\n",
      "dir_num: 6 f1: 0.024691358024691357\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5350\n",
      "Warmup steps: 321\n",
      "100% 50/50 [32:25<00:00, 38.91s/it]\n",
      "dir_num: 7 f1: 0.125\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5500\n",
      "Warmup steps: 330\n",
      "100% 50/50 [34:59<00:00, 41.98s/it]\n",
      "dir_num: 8 f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"6 7 8\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path bert-base-multilingual-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3k_-Wkxkn8I",
    "outputId": "3368479f-03ff-4af9-a82f-965f4dfa1003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 07:19:11.280177: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Downloading: 100% 625/625 [00:00<00:00, 614kB/s]\n",
      "Downloading: 100% 29.0/29.0 [00:00<00:00, 32.4kB/s]\n",
      "Downloading: 100% 996k/996k [00:00<00:00, 2.44MB/s]\n",
      "Downloading: 100% 1.96M/1.96M [00:00<00:00, 2.51MB/s]\n",
      "Downloading: 100% 714M/714M [00:11<00:00, 60.6MB/s]\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5500\n",
      "Warmup steps: 330\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [26:53<00:00, 32.26s/it]\n",
      "dir_num: 9 f1: 0.10309278350515465\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Total steps: 5550\n",
      "Warmup steps: 333\n",
      "100% 50/50 [27:48<00:00, 33.36s/it]\n",
      "dir_num: 10 f1: 0.021621621621621623\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"9 10\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path bert-base-multilingual-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdbQVqM-tjIg",
    "outputId": "76f59c96-0a2d-46fb-8d68-f60a8190f6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 00:51:58.587395: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Downloading: 100% 642/642 [00:00<00:00, 427kB/s]\n",
      "Downloading: 100% 2.00/2.00 [00:00<00:00, 1.43kB/s]\n",
      "Downloading: 100% 1.65M/1.65M [00:00<00:00, 3.11MB/s]\n",
      "Downloading: 100% 112/112 [00:00<00:00, 74.3kB/s]\n",
      "Downloading: 100% 711M/711M [00:19<00:00, 36.7MB/s]\n",
      "Total steps: 4350\n",
      "Warmup steps: 261\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [28:35<00:00, 34.31s/it]\n",
      "dir_num: 0 f1: 0.2235294117647059\n",
      "Total steps: 4300\n",
      "Warmup steps: 258\n",
      "100% 50/50 [28:08<00:00, 33.76s/it]\n",
      "dir_num: 1 f1: 0.1878453038674033\n",
      "Total steps: 4350\n",
      "Warmup steps: 261\n",
      "100% 50/50 [29:20<00:00, 35.22s/it]\n",
      "dir_num: 2 f1: 0.2937062937062937\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"0 1 2\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path DeepPavlov/rubert-base-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K10O_SSpuP6Y",
    "outputId": "63c29835-b36c-46fd-cc75-5221fdaea95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 02:19:07.539466: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Total steps: 4400\n",
      "Warmup steps: 264\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [28:48<00:00, 34.57s/it]\n",
      "dir_num: 3 f1: 0.2482758620689655\n",
      "Total steps: 4300\n",
      "Warmup steps: 258\n",
      "100% 50/50 [28:41<00:00, 34.44s/it]\n",
      "dir_num: 4 f1: 0.16666666666666666\n",
      "Total steps: 4450\n",
      "Warmup steps: 267\n",
      "100% 50/50 [29:44<00:00, 35.70s/it]\n",
      "dir_num: 5 f1: 0.14285714285714282\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"3 4 5\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path DeepPavlov/rubert-base-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aegD5-Mg-vPP",
    "outputId": "74dbe36c-c66e-44df-c273-4152e00b6fd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 08:14:43.748708: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Downloading: 100% 642/642 [00:00<00:00, 617kB/s]\n",
      "Downloading: 100% 2.00/2.00 [00:00<00:00, 1.72kB/s]\n",
      "Downloading: 100% 1.65M/1.65M [00:00<00:00, 4.69MB/s]\n",
      "Downloading: 100% 112/112 [00:00<00:00, 99.4kB/s]\n",
      "Downloading: 100% 711M/711M [00:12<00:00, 59.2MB/s]\n",
      "Total steps: 4150\n",
      "Warmup steps: 249\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [23:03<00:00, 27.67s/it]\n",
      "dir_num: 6 f1: 0.07659574468085105\n",
      "Total steps: 4150\n",
      "Warmup steps: 249\n",
      "100% 50/50 [23:28<00:00, 28.16s/it]\n",
      "dir_num: 7 f1: 0.21333333333333332\n",
      "Total steps: 4200\n",
      "Warmup steps: 252\n",
      "100% 50/50 [24:45<00:00, 29.70s/it]\n",
      "dir_num: 8 f1: 0.03703703703703704\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"6 7 8\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path DeepPavlov/rubert-base-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUwEqzV119Sh",
    "outputId": "4b76a5cb-4e91-462f-b0bb-62367637083f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 09:26:56.246318: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Total steps: 4200\n",
      "Warmup steps: 252\n",
      "  0% 0/50 [00:00<?, ?it/s]/rus_re/project/train.py:50: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
      "100% 50/50 [23:34<00:00, 28.30s/it]\n",
      "dir_num: 9 f1: 0.07511737089201878\n",
      "Total steps: 4300\n",
      "Warmup steps: 258\n",
      "100% 50/50 [24:19<00:00, 29.19s/it]\n",
      "dir_num: 10 f1: 0.0213903743315508\n"
     ]
    }
   ],
   "source": [
    "!python /rus_re/project/train.py  --training_mode cv \\\n",
    "--data_dir /rus_re/cv \\\n",
    "--cv_test_names \"9 10\" \\\n",
    "--transformer_type bert \\\n",
    "--model_name_or_path DeepPavlov/rubert-base-cased \\\n",
    "--save_path /content/drive/'My Drive'/active/rus_re/model \\\n",
    "--train_file all_train \\\n",
    "--dev_file all_valid \\\n",
    "--test_file all_valid \\\n",
    "--train_batch_size 2 \\\n",
    "--max_seq_length 512 \\\n",
    "--test_batch_size 2 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_labels 6 \\\n",
    "--learning_rate 1e-5 \\\n",
    "--max_grad_norm 1.0 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--num_train_epochs 50.0 \\\n",
    "--seed 66 \\\n",
    "--num_class 6"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
