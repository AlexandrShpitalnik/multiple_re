{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"RE_Experiments.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2bd645fbc8004f899cf66aafe0198026":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6ef77117c1ff4b6b8401a7287ac20716","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1ce0419bcbdc49e4b8ca71ddebb8c7ad","IPY_MODEL_b34a5f59cd3f4cda80df033c7775390f"]}},"6ef77117c1ff4b6b8401a7287ac20716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ce0419bcbdc49e4b8ca71ddebb8c7ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1e395c5838a44219afcb6ac01b4b8500","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433286112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433286112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_73953ac4c71b4ad1a2b6add38759106a"}},"b34a5f59cd3f4cda80df033c7775390f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e05c943fc73e45fabb24c21c29c4596d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433M/433M [33:09&lt;00:00, 218kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c3486acd55284cb2a33afd9688546748"}},"1e395c5838a44219afcb6ac01b4b8500":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"73953ac4c71b4ad1a2b6add38759106a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e05c943fc73e45fabb24c21c29c4596d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c3486acd55284cb2a33afd9688546748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c3b3a2b40824ce4a440bc6681d7d730":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_764545bbeede421fbfdee276bfd5f318","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_23e4254e63e04246a25cd07f53203ae2","IPY_MODEL_c54d20921aa149d19fcf8b29a895ef82","IPY_MODEL_6aea7eb587354ff9b6121b8808c4e7a0"]}},"764545bbeede421fbfdee276bfd5f318":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"23e4254e63e04246a25cd07f53203ae2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d3a79630d9e741cc8a90aadf7216563a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a64515e63c4045b4b2ff78c094056acb"}},"c54d20921aa149d19fcf8b29a895ef82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e183005a2474474abefe9961d43f2e57","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":49,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":49,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc616427c97a46ce92b6124f1d1694c3"}},"6aea7eb587354ff9b6121b8808c4e7a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dc2dbcfc4c19402da618a49827023ef1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 49.0/49.0 [00:00&lt;00:00, 1.06kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df7e35082418414282f9baee6f6103ad"}},"d3a79630d9e741cc8a90aadf7216563a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a64515e63c4045b4b2ff78c094056acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e183005a2474474abefe9961d43f2e57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dc616427c97a46ce92b6124f1d1694c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dc2dbcfc4c19402da618a49827023ef1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"df7e35082418414282f9baee6f6103ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2f9a6a254f8e420a90ced8569941d046":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0014d0cc1e004e3c8833c6a7a98ab73e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0f1f766bdda94da4a419f09dee9e7fd4","IPY_MODEL_ab8a3f0bfae84482975a761719dde1e8","IPY_MODEL_a963c60cfbe446d5ba1da70de3983d64"]}},"0014d0cc1e004e3c8833c6a7a98ab73e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f1f766bdda94da4a419f09dee9e7fd4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b77b98168a324f66b73a11c7ab5cc7e1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4eb289cfaa0b43a4a4fd1a7acffb609d"}},"ab8a3f0bfae84482975a761719dde1e8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d2effc68a8a64b3bbb8099965bf498e4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":462,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":462,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_88acb2238f484ac6a2ca4a5fd9fcf8c8"}},"a963c60cfbe446d5ba1da70de3983d64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8866fc03f0b647f18fb0765ff7f9d83b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 462/462 [00:00&lt;00:00, 13.3kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f9d9371f40b48ee8244d28645c95586"}},"b77b98168a324f66b73a11c7ab5cc7e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4eb289cfaa0b43a4a4fd1a7acffb609d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d2effc68a8a64b3bbb8099965bf498e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"88acb2238f484ac6a2ca4a5fd9fcf8c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8866fc03f0b647f18fb0765ff7f9d83b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f9d9371f40b48ee8244d28645c95586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"297a30336580447ba28131e3b0558b2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_443ee70cc6f1427e9ca685cc76ea5d42","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9c542109705f4cf6bffc129c9bd3d4c2","IPY_MODEL_a02918aa89184f7aa15dd7cb10813924","IPY_MODEL_be603bf814fa457596745113cf1e7fbc"]}},"443ee70cc6f1427e9ca685cc76ea5d42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9c542109705f4cf6bffc129c9bd3d4c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9b5a87f90bfb43bbb6a639d99f2a633d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_517018b6d7ce4c4496cf7f9fe2ef72c1"}},"a02918aa89184f7aa15dd7cb10813924":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6047570a314342c9ae4da45203a2df8d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7bcec5b7943941b5953a47e03a44b48a"}},"be603bf814fa457596745113cf1e7fbc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e4c9c44e02344c7ea8497742e19c4e28","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 213k/213k [00:00&lt;00:00, 867kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_412c05d48b3748729752e2fb3bb00ced"}},"9b5a87f90bfb43bbb6a639d99f2a633d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"517018b6d7ce4c4496cf7f9fe2ef72c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6047570a314342c9ae4da45203a2df8d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7bcec5b7943941b5953a47e03a44b48a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4c9c44e02344c7ea8497742e19c4e28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"412c05d48b3748729752e2fb3bb00ced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6dc2230575d749f2a3e191356b6073fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b68c0b62b706483f9c1a65b0c5f220ba","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d4fcb787e3824a96b401ec9108452d30","IPY_MODEL_dad8dffe6eac406fa94cc55c1081ad4a","IPY_MODEL_9fdaf8fcf7a345adb9f8da1a46bd8f3b"]}},"b68c0b62b706483f9c1a65b0c5f220ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d4fcb787e3824a96b401ec9108452d30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b88c1cf5e2a7438cbadb91729ad2f5a7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d92b97b1e2494876b538cbc3a22920e9"}},"dad8dffe6eac406fa94cc55c1081ad4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0f50472558bf4cd9821eb5cd182773d4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2636571d9a541a8810b9ff6e328d38d"}},"9fdaf8fcf7a345adb9f8da1a46bd8f3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1aba70cde2bd4025b4c24c16a1d80788","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 112/112 [00:00&lt;00:00, 2.54kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8006bc1bc04f4ffe9f3630370ed61907"}},"b88c1cf5e2a7438cbadb91729ad2f5a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d92b97b1e2494876b538cbc3a22920e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f50472558bf4cd9821eb5cd182773d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c2636571d9a541a8810b9ff6e328d38d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1aba70cde2bd4025b4c24c16a1d80788":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8006bc1bc04f4ffe9f3630370ed61907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a691fcc2ce774d70952c70c738a1fdea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fcfecf1023bd4b54849f555a52d33e6d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ce09696c544f47f9b23da7725648125b","IPY_MODEL_d38ada3506d34bd590bb7869c6af0e53","IPY_MODEL_f003596bb6b04131a9a29463deed5c7f"]}},"fcfecf1023bd4b54849f555a52d33e6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce09696c544f47f9b23da7725648125b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_569fec6048d74671884eed0678c89f34","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5af4d332c2314fa1aeec574b16e4ff6c"}},"d38ada3506d34bd590bb7869c6af0e53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5c4f69ac47364bbd9c8a680122485c4a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":433286112,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433286112,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5817ff0b218345639d5d265bc2bd6a17"}},"f003596bb6b04131a9a29463deed5c7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e750f161c71d49918050f0f50efbc2e3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433M/433M [00:12&lt;00:00, 34.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_889eaf1c64d24363be801bfe115f1fd3"}},"569fec6048d74671884eed0678c89f34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5af4d332c2314fa1aeec574b16e4ff6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c4f69ac47364bbd9c8a680122485c4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5817ff0b218345639d5d265bc2bd6a17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e750f161c71d49918050f0f50efbc2e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"889eaf1c64d24363be801bfe115f1fd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlcM--GrSYyY","executionInfo":{"status":"ok","timestamp":1630017518756,"user_tz":-180,"elapsed":13009,"user":{"displayName":"Александр Шпитальник","photoUrl":"","userId":"03123787254571160547"}},"outputId":"6f93fa66-20b8-4bae-a1ac-a7daa46c9859"},"source":["! pip install transformers\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import sys\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler#, get_linear_schedule_with_warmup\n","#from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from sklearn.metrics import recall_score, precision_score\n","from tqdm import trange\n","\n","\n","from transformers import AutoTokenizer, AutoModel\n","from transformers import BertForSequenceClassification\n","from torch.optim import Adam\n","from transformers import get_linear_schedule_with_warmup, AdamW"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 41.7 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 37.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 33.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rnQhXNb2Sf0u","executionInfo":{"status":"ok","timestamp":1630017557884,"user_tz":-180,"elapsed":39134,"user":{"displayName":"Александр Шпитальник","photoUrl":"","userId":"03123787254571160547"}},"outputId":"95e584c9-b58e-42ad-b4cc-c8e89396742a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","! cp -r -f /content/drive/'My Drive'/msu/re_from_bioBert /content/\n","import sys\n","\n","sys.path.append('/content/msu/re_from_bioBert/')\n","dir_name = '/content/re_from_bioBert/'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xGohRsE3lQmH","executionInfo":{"status":"ok","timestamp":1630017557885,"user_tz":-180,"elapsed":14,"user":{"displayName":"Александр Шпитальник","photoUrl":"","userId":"03123787254571160547"}}},"source":["def fit(model, x, y, n_epochs, lr, batch_size=32):\n","    masks = [[float(i != 0.0) for i in ii] for ii in x]\n","    \n","    t_inputs = torch.tensor(x)\n","    t_labels = torch.tensor(y)\n","    t_masks = torch.tensor(masks)\n","\n","    train_data = TensorDataset(t_inputs, t_masks, t_labels)\n","    train_sampler = SequentialSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    model.cuda()\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay  = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0}\n","    ]\n","    \n","    optimizer = AdamW(\n","    optimizer_grouped_parameters,\n","    lr=lr,\n","    eps=1e-6)\n","\n","    epochs = n_epochs\n","    max_grad_norm = 1.5\n","\n","    # Total number of training steps is number of batches * number of epochs.\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler.\n","\n","\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=0.1 * total_steps,\n","        num_training_steps=total_steps)\n","    \n","    model.train()\n","    for _ in trange(epochs, desc=\"Epoch\"):\n","        total_loss = 0\n","\n","        for step, batch in enumerate(train_dataloader):\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            model.zero_grad()\n","        \n","            outputs = model(b_input_ids, token_type_ids=None,\n","                                  attention_mask=b_input_mask, labels=b_labels)\n","        \n","            loss = outputs[0]\n","            loss.backward()\n","            total_loss += loss.item()\n","\n","            #torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","            optimizer.step()\n","            scheduler.step()\n","        print(' Loss: ', total_loss)    \n","\n","\n","def predict(model, x, batch_size=32):\n","    masks = [[float(i != 0.0) for i in ii] for ii in x]\n","\n","    t_inputs = torch.tensor(x)\n","    t_masks = torch.tensor(masks)\n","\n","    test_data = TensorDataset(t_inputs, t_masks)\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","\n","    model.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    predictions = []\n","\n","    for batch in test_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask = batch\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","        outputs = outputs[0].detach().cpu().numpy()\n","        \n","        batch_labels = np.argmax(outputs, axis=1).tolist()\n","        predictions += batch_labels\n","    return predictions\n","\n","def get_data_from_df(df, tokenizer):\n","    lbls, raw_txt = df.lbl.values, df.txt.values\n","    txt = ['[CLS] ' + i + ' [SEP]' for i in raw_txt]\n","    res = []\n","    for sent in txt:\n","      tokens = tokenizer.tokenize(sent)\n","      tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n","      while len(tokens_ids) < MAX_LEN:\n","        tokens_ids.append(0)\n","      res.append(tokens_ids)\n","    return res, lbls\n","    \n","\n","def cv_iteration(model, tokenizer, data_dir, cur_it, n_epochs, lr, batch_size=32):\n","    train_df = pd.read_csv(data_dir + '/'+ str(cur_it+1) + '/train.tsv', sep = '\\t', names = ['txt', 'lbl'])\n","    train_tokens, train_lbls = get_data_from_df(train_df, tokenizer)\n","    train_lbls = [int(i) for i in train_lbls]\n","    \n","    test_df = pd.read_csv(data_dir + '/'+ str(cur_it+1) + '/test.tsv', sep = '\\t', names = ['txt', 'lbl'])\n","    test_tokens, test_lbls = get_data_from_df(test_df, tokenizer)\n","    test_tokens, test_lbls = test_tokens[1:], test_lbls[1:]\n","    test_lbls = [int(i) for i in test_lbls]\n","    \n","    \n","    fit(model, train_tokens, train_lbls, n_epochs, lr, batch_size)\n","    preds = predict(model, test_tokens, batch_size)\n","    #print(np.bincount(np.array(train_tokens)), np.bincount(np.array(preds)))\n","\n","    precision, recall = precision_score(test_lbls, preds), recall_score(test_lbls, preds)\n","    f1 = 2 * (precision * recall) / (precision + recall)\n","    return precision, recall, f1\n","\n","\n","def get_cv_scores(model_name, tokenizer, data_dir, lr,  n_epochs, batch_size=32, st_from=0):\n","    prescision_s, recall_s, f1_s = [], [], []\n","    for i in range(st_from, 10):\n","        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","        p, r, f = cv_iteration(model, tokenizer, data_dir, i, n_epochs, lr, batch_size)\n","        print('P: ', p, 'R: ', r, 'F1: ', f)\n","        prescision_s.append(p)\n","        recall_s.append(r)\n","        f1_s.append(f)\n","    return sum(prescision_s) / len(prescision_s), sum(recall_s) / len(recall_s), sum(f1_s) / len(f1_s) \n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"tamUnMVGlRhp","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["2bd645fbc8004f899cf66aafe0198026","6ef77117c1ff4b6b8401a7287ac20716","1ce0419bcbdc49e4b8ca71ddebb8c7ad","b34a5f59cd3f4cda80df033c7775390f","1e395c5838a44219afcb6ac01b4b8500","73953ac4c71b4ad1a2b6add38759106a","e05c943fc73e45fabb24c21c29c4596d","c3486acd55284cb2a33afd9688546748"]},"executionInfo":{"status":"ok","timestamp":1608150227601,"user_tz":-180,"elapsed":3952051,"user":{"displayName":"Александр Шпитальник","photoUrl":"","userId":"03123787254571160547"}},"outputId":"6713dc92-305e-435d-c60b-879c323dafa4"},"source":["MAX_LEN = 150\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n","model = \"dmis-lab/biobert-v1.1\"\n","data_dir = \"/content/re_from_bioBert/GAD\"\n","res = get_cv_scores(model, tokenizer, data_dir, 2e-5, 3, 32)\n","p, r, f = res\n","print('P: ', p, 'R: ', r, 'F1: ', f)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bd645fbc8004f899cf66aafe0198026","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433286112.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:02<04:04, 122.36s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  91.39673164486885\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:09<02:03, 123.77s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  68.3507828116417\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:17<00:00, 125.67s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.80413669347763\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7492795389048992 R:  0.9252669039145908 F1:  0.8280254777070065\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:15, 127.61s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  91.32923457026482\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:15<02:07, 127.58s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  65.1553760021925\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:22<00:00, 127.41s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.22057132422924\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7835365853658537 R:  0.9178571428571428 F1:  0.8453947368421053\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:14, 127.46s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  92.3953418135643\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:14<02:07, 127.46s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  66.82543221116066\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:22<00:00, 127.43s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  52.64888007938862\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.788961038961039 R:  0.8678571428571429 F1:  0.8265306122448981\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:14, 127.44s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  91.7310399711132\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:14<02:07, 127.34s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  66.5868114233017\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:21<00:00, 127.31s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  51.39080773293972\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.739612188365651 R:  0.9535714285714286 F1:  0.8330733229329175\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:15, 127.63s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  91.4558767080307\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:14<02:07, 127.48s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  64.86458984017372\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:22<00:00, 127.41s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  51.01508919149637\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7386018237082067 R:  0.8678571428571429 F1:  0.7980295566502463\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:15, 127.57s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  91.1013668179512\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:14<02:07, 127.48s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  63.5058980435133\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:22<00:00, 127.46s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  50.84156250953674\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7507418397626113 R:  0.9035714285714286 F1:  0.820097244732577\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:14, 127.48s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  90.09555000066757\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:14<02:07, 127.39s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  64.50710928440094\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:21<00:00, 127.32s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  51.09539006650448\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7363344051446945 R:  0.8178571428571428 F1:  0.7749576988155669\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:15, 127.62s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  93.0159969329834\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:14<02:07, 127.51s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  64.93791165947914\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:22<00:00, 127.49s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  51.85404919087887\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7734138972809668 R:  0.9142857142857143 F1:  0.8379705400981996\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:14, 127.33s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  90.28923550248146\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:14<02:07, 127.41s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  66.6409080028534\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:22<00:00, 127.39s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  50.316844791173935\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7230320699708455 R:  0.8857142857142857 F1:  0.7961476725521668\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  33%|███▎      | 1/3 [02:07<04:15, 127.53s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  90.31613659858704\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [04:15<02:07, 127.59s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  66.50431564450264\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [06:22<00:00, 127.46s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  52.94078643620014\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7155172413793104 R:  0.8892857142857142 F1:  0.7929936305732483\n","P:  0.7499030628844078 R:  0.8943124046771734 F1:  0.8153220493148933\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8c3b3a2b40824ce4a440bc6681d7d730","764545bbeede421fbfdee276bfd5f318","23e4254e63e04246a25cd07f53203ae2","c54d20921aa149d19fcf8b29a895ef82","6aea7eb587354ff9b6121b8808c4e7a0","d3a79630d9e741cc8a90aadf7216563a","a64515e63c4045b4b2ff78c094056acb","e183005a2474474abefe9961d43f2e57","dc616427c97a46ce92b6124f1d1694c3","dc2dbcfc4c19402da618a49827023ef1","df7e35082418414282f9baee6f6103ad","2f9a6a254f8e420a90ced8569941d046","0014d0cc1e004e3c8833c6a7a98ab73e","0f1f766bdda94da4a419f09dee9e7fd4","ab8a3f0bfae84482975a761719dde1e8","a963c60cfbe446d5ba1da70de3983d64","b77b98168a324f66b73a11c7ab5cc7e1","4eb289cfaa0b43a4a4fd1a7acffb609d","d2effc68a8a64b3bbb8099965bf498e4","88acb2238f484ac6a2ca4a5fd9fcf8c8","8866fc03f0b647f18fb0765ff7f9d83b","4f9d9371f40b48ee8244d28645c95586","297a30336580447ba28131e3b0558b2b","443ee70cc6f1427e9ca685cc76ea5d42","9c542109705f4cf6bffc129c9bd3d4c2","a02918aa89184f7aa15dd7cb10813924","be603bf814fa457596745113cf1e7fbc","9b5a87f90bfb43bbb6a639d99f2a633d","517018b6d7ce4c4496cf7f9fe2ef72c1","6047570a314342c9ae4da45203a2df8d","7bcec5b7943941b5953a47e03a44b48a","e4c9c44e02344c7ea8497742e19c4e28","412c05d48b3748729752e2fb3bb00ced","6dc2230575d749f2a3e191356b6073fb","b68c0b62b706483f9c1a65b0c5f220ba","d4fcb787e3824a96b401ec9108452d30","dad8dffe6eac406fa94cc55c1081ad4a","9fdaf8fcf7a345adb9f8da1a46bd8f3b","b88c1cf5e2a7438cbadb91729ad2f5a7","d92b97b1e2494876b538cbc3a22920e9","0f50472558bf4cd9821eb5cd182773d4","c2636571d9a541a8810b9ff6e328d38d","1aba70cde2bd4025b4c24c16a1d80788","8006bc1bc04f4ffe9f3630370ed61907","a691fcc2ce774d70952c70c738a1fdea","fcfecf1023bd4b54849f555a52d33e6d","ce09696c544f47f9b23da7725648125b","d38ada3506d34bd590bb7869c6af0e53","f003596bb6b04131a9a29463deed5c7f","569fec6048d74671884eed0678c89f34","5af4d332c2314fa1aeec574b16e4ff6c","5c4f69ac47364bbd9c8a680122485c4a","5817ff0b218345639d5d265bc2bd6a17","e750f161c71d49918050f0f50efbc2e3","889eaf1c64d24363be801bfe115f1fd3"]},"id":"pgHZMp8KnJfP","executionInfo":{"status":"ok","timestamp":1630021753091,"user_tz":-180,"elapsed":4195216,"user":{"displayName":"Александр Шпитальник","photoUrl":"","userId":"03123787254571160547"}},"outputId":"383c07af-1f94-49e8-9aaf-ad041fc2364f"},"source":["# st\n","MAX_LEN = 150\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n","model = \"dmis-lab/biobert-v1.1\"\n","data_dir = \"/content/re_from_bioBert/GAD\"\n","res = get_cv_scores(model, tokenizer, data_dir, 2e-5, 5, 16)\n","p, r, f = res\n","print('P: ', p, 'R: ', r, 'F1: ', f)"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c3b3a2b40824ce4a440bc6681d7d730","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f9a6a254f8e420a90ced8569941d046","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/462 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"297a30336580447ba28131e3b0558b2b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6dc2230575d749f2a3e191356b6073fb","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a691fcc2ce774d70952c70c738a1fdea","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:22<05:28, 82.05s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  176.32395718991756\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:43<04:05, 81.98s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  120.85679131746292\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:05<02:43, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  86.35665542632341\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:27<01:21, 81.94s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  57.66482341475785\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:49<00:00, 81.96s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  41.220343900844455\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.782608695652174 R:  0.896797153024911 F1:  0.835820895522388\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:22<05:28, 82.04s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  180.4209145605564\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:44<04:06, 82.01s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  127.1702843979001\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:05<02:43, 81.97s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  93.14453118667006\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:27<01:21, 81.99s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  66.3264816943556\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:50<00:00, 82.02s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  51.04550935141742\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7716049382716049 R:  0.8928571428571429 F1:  0.8278145695364238\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:22<05:28, 82.09s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  178.9402416497469\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:44<04:06, 82.10s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  124.95850194990635\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:06<02:44, 82.08s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  91.67490712180734\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:28<01:22, 82.09s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  68.26392049528658\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:50<00:00, 82.08s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  51.353994777426124\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.802675585284281 R:  0.8571428571428571 F1:  0.8290155440414508\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:22<05:28, 82.03s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  184.43567755818367\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:44<04:06, 82.04s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  136.57060995697975\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:06<02:44, 82.05s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  98.22047037258744\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:28<01:22, 82.03s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  70.72862117923796\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:50<00:00, 82.04s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.94600309431553\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7832817337461301 R:  0.9035714285714286 F1:  0.8391376451077944\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:22<05:28, 82.10s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  176.38783939182758\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:44<04:06, 82.04s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  127.27927298098803\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:06<02:44, 82.04s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  93.88187519833446\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:28<01:22, 82.02s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  68.77377816475928\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:50<00:00, 82.04s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.00340067408979\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7740863787375415 R:  0.8321428571428572 F1:  0.8020654044750429\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:22<05:28, 82.06s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  184.52356451749802\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:44<04:06, 82.02s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  125.17060396075249\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:06<02:44, 82.02s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  92.56694284081459\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:28<01:22, 82.01s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  68.60872922092676\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:49<00:00, 81.99s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  52.74399168230593\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7548387096774194 R:  0.8357142857142857 F1:  0.7932203389830508\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:27, 81.94s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  180.73018236458302\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:43<04:05, 81.97s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  123.65838228166103\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:05<02:43, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  86.01882552169263\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:27<01:21, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  63.55241402890533\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:49<00:00, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  44.19360501598567\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7551020408163265 R:  0.7928571428571428 F1:  0.7735191637630662\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:22<05:28, 82.02s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  181.5578817129135\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:43<04:05, 81.98s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  128.65678361058235\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:05<02:43, 81.94s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  89.10504324361682\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:27<01:21, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  60.94657200574875\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:49<00:00, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  44.76673574373126\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7767584097859327 R:  0.9071428571428571 F1:  0.8369028006589786\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:27, 81.98s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  178.11184349656105\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:43<04:05, 82.00s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  124.37874429672956\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:05<02:43, 81.97s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  93.60565999895334\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:27<01:21, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  68.57284789159894\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:49<00:00, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.41627797856927\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.77491961414791 R:  0.8607142857142858 F1:  0.8155668358714044\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:27, 81.94s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  178.43343782424927\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:43<04:05, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  125.47255045175552\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:05<02:43, 81.95s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  92.59125239402056\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:27<01:21, 81.94s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  70.54833816550672\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:49<00:00, 81.94s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.16209271363914\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.75 R:  0.8571428571428571 F1:  0.7999999999999999\n","P:  0.772587610611932 R:  0.8636082867310625 F1:  0.81530631979596\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XkDzovGoSgBu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629681414967,"user_tz":-180,"elapsed":11351507,"user":{"displayName":"Александр Шпитальник","photoUrl":"","userId":"03123787254571160547"}},"outputId":"be04ba56-fda5-43b2-9ca5-e52f0c5b570c"},"source":["MAX_LEN = 150\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-large-cased-v1.1\")\n","model = \"dmis-lab/biobert-large-cased-v1.1\"\n","data_dir = \"/content/re_from_bioBert/GAD\"\n","res = get_cv_scores(model, tokenizer, data_dir, 2e-5, 5, 16)\n","p, r, f = res\n","print('P: ', p, 'R: ', r, 'F1: ', f)"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:52, 238.01s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" Loss:  176.25494734942913\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:55<11:53, 237.93s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" Loss:  129.74050809070468\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:53<07:55, 237.90s/it]"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":[" Loss:  93.455983877182\n"]},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:51<03:57, 237.85s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  59.803071587346494\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:49<00:00, 237.89s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  40.814359695184976\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7954545454545454 R:  0.8718861209964412 F1:  0.831918505942275\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:52, 238.04s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  190.2874919772148\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:56<11:54, 238.10s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  142.99986818432808\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:54<07:56, 238.16s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  107.64651624113321\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:52<03:58, 238.21s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  73.6571749560535\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:51<00:00, 238.23s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  47.186623906716704\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7781456953642384 R:  0.8392857142857143 F1:  0.8075601374570447\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:53, 238.46s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  182.04336892068386\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:56<11:55, 238.46s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  145.65606516599655\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:55<07:56, 238.49s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  113.03258479759097\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:54<03:58, 238.54s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  79.63784670084715\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:52<00:00, 238.55s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  52.44506885576993\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.8014705882352942 R:  0.7785714285714286 F1:  0.7898550724637681\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:55, 238.77s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  177.3434747159481\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:57<11:56, 238.68s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  130.87819084525108\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:56<07:57, 238.70s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  95.72817789390683\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:54<03:58, 238.76s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  64.88852996379137\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:53<00:00, 238.74s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  44.62468160595745\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.8288590604026845 R:  0.8821428571428571 F1:  0.8546712802768165\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:55, 238.88s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  179.05990405380726\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:57<11:56, 238.82s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  131.128272280097\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:56<07:57, 238.81s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  98.06193000078201\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:55<03:58, 238.81s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  70.879180053249\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:54<00:00, 238.81s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  48.497114586643875\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7896551724137931 R:  0.8178571428571428 F1:  0.8035087719298246\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:55, 238.89s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  202.87380266189575\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:57<11:56, 238.84s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  160.95099292695522\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:56<07:57, 238.81s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  128.78586754202843\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:55<03:58, 238.82s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  95.28491580486298\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:54<00:00, 238.83s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  67.2579286545515\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7304347826086957 R:  0.9 F1:  0.8064\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:55, 238.79s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  177.69927996397018\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:57<11:56, 238.81s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  128.2086355164647\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:56<07:57, 238.82s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  90.54151129722595\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:55<03:58, 238.87s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  60.523281555622816\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:54<00:00, 238.84s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  39.10336833447218\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7671232876712328 R:  0.8 F1:  0.7832167832167832\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:55, 238.87s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  208.26389780640602\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:57<11:56, 238.84s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  211.67049485445023\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:56<07:57, 238.83s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  211.03332245349884\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:55<03:58, 238.84s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  209.98034101724625\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:54<00:00, 238.84s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  209.65147268772125\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.525328330206379 R:  1.0 F1:  0.6888068880688807\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:55, 238.88s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  178.43371632695198\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:57<11:56, 238.81s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  139.2711308375001\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:56<07:57, 238.83s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  102.84789847955108\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:55<03:58, 238.85s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  73.6538950484246\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:54<00:00, 238.84s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  50.01313385087997\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7547169811320755 R:  0.8571428571428571 F1:  0.8026755852842811\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-large-cased-v1.1 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-large-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [03:58<15:55, 238.92s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  198.82134228944778\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [07:57<11:56, 238.86s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  211.2990289926529\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [11:56<07:57, 238.87s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  210.0167389512062\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [15:55<03:58, 238.88s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  209.78010261058807\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [19:54<00:00, 238.88s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  208.9656002521515\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.5263157894736842 R:  1.0 F1:  0.6896551724137931\n","P:  0.7297504232962622 R:  0.8746886120996441 F1:  0.7858268197053467\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UacE8Kj8Nqh3","executionInfo":{"status":"ok","timestamp":1629662821596,"user_tz":-180,"elapsed":4142010,"user":{"displayName":"Александр Шпитальник","photoUrl":"","userId":"03123787254571160547"}},"outputId":"3bfae3b4-13ee-41e3-bd86-6dc2ac1a9e1b"},"source":["MAX_LEN = 150\n","\n","model = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","data_dir = \"/content/re_from_bioBert/GAD\"\n","res = get_cv_scores(model, tokenizer, data_dir, 2e-5, 5, 16)\n","p, r, f = res\n","print('P: ', p, 'R: ', r, 'F1: ', f)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.20s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  177.69122552871704\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  125.12045377492905\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  93.97316182032228\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  72.40580599196255\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.53787698969245\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7766990291262136 R:  0.8540925266903915 F1:  0.8135593220338982\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  179.9779968559742\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  128.7628919184208\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  100.03299463167787\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  76.56411984562874\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  61.29839794896543\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7692307692307693 R:  0.8928571428571429 F1:  0.8264462809917357\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  178.84841883182526\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  132.09764552116394\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  94.74046341329813\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  70.12284239009023\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  52.775863917544484\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7684887459807074 R:  0.8535714285714285 F1:  0.8087986463620982\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  175.45140942931175\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.16s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  123.89612405747175\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.16s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  91.39078196510673\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  68.2712718937546\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  53.303023900836706\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7719298245614035 R:  0.9428571428571428 F1:  0.8488745980707395\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  181.52083586156368\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  134.7963247820735\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  103.33657451719046\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.16s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  77.7221940420568\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  61.52654386870563\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.780952380952381 R:  0.8785714285714286 F1:  0.8268907563025211\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  184.06574335694313\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  132.027650937438\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.14s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  96.04499527439475\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  70.65857911482453\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  57.04331450164318\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7784810126582279 R:  0.8785714285714286 F1:  0.825503355704698\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.18s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  183.41467152535915\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.16s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  123.75978025048971\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.16s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  89.37801592797041\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  66.68946928344667\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  50.46974304597825\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7656765676567657 R:  0.8285714285714286 F1:  0.7958833619210979\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  180.46789702773094\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  127.33587038516998\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.14s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  98.28983097523451\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.14s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  72.70795572176576\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  56.207456255331635\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7522388059701492 R:  0.9 F1:  0.8195121951219512\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:24, 81.17s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  178.92142802476883\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  126.3027770742774\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  96.20940491184592\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  72.49244609102607\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:45<00:00, 81.15s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  57.56838612817228\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7591463414634146 R:  0.8892857142857142 F1:  0.8190789473684209\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|██        | 1/5 [01:21<05:25, 81.43s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  176.32146929204464\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:42<04:03, 81.26s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  124.81810785084963\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [04:03<02:42, 81.21s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  95.07664704322815\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [05:24<01:21, 81.19s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  74.10094687342644\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [06:46<00:00, 81.21s/it]"],"name":"stderr"},{"output_type":"stream","text":[" Loss:  59.3399417065084\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["P:  0.7360703812316716 R:  0.8964285714285715 F1:  0.8083735909822866\n","P:  0.7658913858831704 R:  0.8814806812404676 F1:  0.8192921054859447\n"],"name":"stdout"}]}]}